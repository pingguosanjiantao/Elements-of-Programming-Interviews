网页排名算法是指对一个大量重要网页链接的网页的排名。
算法基本上如下：
(1)新建一个基于网页链接结构的矩阵A，具体来说，Aij=a/dj，如果网页j链接到网页i，这里dj表示从网页j链接的不同页面数量。
(2)寻找一个满足X=e[1]+(1-e)AX的X。这里e是一个常量，比如1/7，[1]表示1的列向量，X[i]表示第i个页面的排名。
最常用的一个方法是使用一个X的初始值来解上述方程，其中每一个组成是1/n，n是页面的数量，然后执行下面的迭代：Xk=e[1]+(1-e)AXk-1，当Xk收敛时迭代终止，例如当Xk和Xk+1的差值小于某个阈值时。
设计一个系统，能满足在合理时间内对百亿的网页进行排名。
*提示：这个过程需在所有机器上执行，合适的数据结构将会简化计算*
答：由于网页图有十亿级的顶点，它大部分是一个稀疏的图，最好是将图表述为邻接列表。创建邻接列表可能需要一定的运算时间。这依赖于信息是怎样收集的。通常，图是通过下载网页上的页面然后提取页面中的超链接来构建的。由于页面的URL会很长，使用hashCode来标识是一个不错的选择。
页面排序算法最耗时的部分应该是矩阵的乘法。通过，我们是不可能在单机中保存整个图的信息。
下面是两个解决这个问题的方法：
- 基于硬盘的排序——我们将列X保存在内容中，每次加载一列。处理i列仅需要循环j当在Aij不是0的时候，将AijXj加到Xj上。这个方法的优势是如果单机能满足列向量的存储，那么整个运算就都可以在单机上执行。如果只是使用单机，依赖于硬盘的话，那这个方法是很慢的。
- 图分区——我们用n个服务器，将定点（网页）拆分成n个集合。可以通过哈希计算得到分区集合。这样的话容易找到顶点映射的机器。在给定分区后，每个机器加载它对应的顶点，以及出边到RAM。
每个机器同时也加载它负责数据的那一部分网页排名。然后每个机器在本地做矩阵乘法。
由于每个机器存在着与其他机器持有顶点连接的边，所以结果向量需要包括那些不是本机持有的顶点的非空集合。
在本地乘法的最后需要发送更新信号给其他主机，这样这些值就被相加起来。
这个方法的优势在于它可以计算任意大小的图。

网页排序在单机上计算维基百科中的几百万的图需要几分钟。需要进行大概70个迭代来覆盖整个图。
据说，需要200个迭代才能覆盖整个互联网的网页。
当考虑网页图的扩展性时，网页排序必须在成千上万的机器上运行。在这个场景下，是非常有可能出现一个机器跑失败了，例如没电了。目前被广泛使用的map-reduce架构在并行计算和容错方面都有良好的表现。粗略的讲，在分布式文件系统中，容错可以通过数据拷贝实现，当某些机子没有响应时，可以在主机上重新分配任务。