设计一个高效的算法：输入为一个文本文件的集合，输出为有大量相似内容的文件对
*提示：设计一个hash方法，使得hash值S[i:i+k-1](其中i=0,1,2...)递增*
答：我们把每个文件当作一个字符串，那么俩文件有大量相似可以描述为有长度为k的子串相同，其中k是一个相似性参数。（一会儿我们将讨论深入讨论这个模型的正确性）
假设Ii为第i个文件的字符串长度，对于每个字符串，我们计算Ii-k+1个哈希值，其中每k个长度的子串计算一次。
我们把这些哈希值插入到一个大的哈希表G中，value表示表示每个码对应的文件，以及对应的子串的偏移位置。当发生哈希碰撞时就表示两个k长度的子串可能是相同。
由于我们计算了每个k长度子串的哈希值，所以需要考虑当我们删除或新增一个字符时哈希方法的增量更新问题的效率。7.13展示了一个哈希方法。
还有，可以考虑在G中使用大量的槽，来降低碰撞概率。（此处省略图片）。如果相对于字符串长度来说k很小时，以及G的槽远小于字符串的特性是，很小可能导致碰撞。
如果我们不需要返回一个非常精确的结果的话，我们可以只考虑子串的集合来减少存储空间。例如，在最后b个位中有0的哈希码。这平均下来，我们只需要考虑子串总数的1/2^b。（假设哈希方法使用了自动扩展key）
上述方法可能导致错误的结果。例如，如果每个文件是一个html页面，所有页面都有一个长度为k的或者能够有大量一致长度的公共嵌入式脚本。
我们可以考虑通过预处理，例如解析文件，然后移除文件头（这个可能需要多次过滤，有时还需要手动执行）。
同时，这个方法也可能是无效的，例如有些文件可能相似的，虽然事实上是一样的，但是可能索引不一样。（这有可能是代码块移动）
对于正常的索引来说，我们可以把问题转换会寻找公共子串问题。
我们描述的这个方法当文件非常大的时候是很有效的。在特殊情况下，使用map-reduce方法可以实现大哈希表G的跨服务器的影响。